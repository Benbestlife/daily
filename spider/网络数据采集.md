***
## urllib
* urllib.request
    for opening and reading URLs
* urllib.error
    containing the exceptions raised by urllib.request
* urllib.parse
    for parsing URLs
* urllib.robotparser
    for parsing robots.txt files

### urllib模块中的方法

可先构建一个Request对象

1. urllib.urlopen(url[,data[,proxies]])
    打开一个url的方法，返回一个文件对象，然后可以进行类似文件对象的操作。
**urlopen返回对象提供的方法**
* read() , readline() ,readlines() , fileno() , close()
    这些方法的使用方式与文件对象完全一样
* info()
    返回一个httplib.HTTPMessage对象，表示远程服务器返回的头信息
* getcode()
    返回Http状态码。如果是http请求，200请求成功完成;404网址未找到
* geturl()
    返回请求的url

2. urllib.urlretrieve(url[,filename[,reporthook[,data]]])
urlretrieve方法将url定位到的html文件下载到你本地的硬盘中。
如果不指定filename，则会存为临时文件。
urlretrieve()返回一个二元组(filename,mine_hdrs)

3. urllib.urlcleanup()
清除由于urllib.urlretrieve()所产生的缓存

4. urllib.quote(url)和urllib.quote_plus(url)
将url数据获取之后，并将其编码，从而适用与URL字符串中，使其能被打印和被web服务器接受。

5. urllib.unquote(url)和urllib.unquote_plus(url)
与4的函数相反。

6. urllib.urlencode(query)
将URL中的键值对以连接符&划分，这里可以与urlopen结合以实现post方法和get方法：
**get方法**
`
params = urlencode({'spam':1, 'eggs':2, 'bacon':0})
geturl = "http://python.org/query?{}".formate(params)
request = Request(geturl)
f = urlopen(request)
`
**post方法**
`
parmas = urlencode({'spam':1, 'eggs':2, 'bacon':0})
request = Request("http://python.org/query", parmas)
f = urlopen(request)
`

***
## BeautifulSoup

**BeautifulSoup 对象**
* BeautifulSoup 对象
* 标签 Tag 对象
BeautifulSoup 对象通过 find 和 findAll ,或者直接调用子标签获取的一列对象或单个对象
* NavigableString 对象
用来表示标签里的文字,不是标签(有些函数可以操作和生成 NavigableString 对象,而不是标签对象)。
* Comment 对象
用来查找 HTML 文档的注释标签

**get_text与保留标签**
通常在你准备打印、存储和操作数据时,应该最后才使用 .get_text()。
一般情况下,应尽可能地保留 HTML 文档的标签结构。

**find()和find_all()**
* findAll(tag, attributes, recursive, text, limit, keywords)
* find(tag, attributes, recursive, text, keywords)

**导航树(Navigating Trees)**
* 子标签 children() 和 后代标签 descendants()
* 兄弟标签 next_siblings() 和 previous_siblings()
对象不能把自己作为兄弟标签;这个函数只调用后面的兄弟标签.
next_sibling 和 previous_sibling 函数 返回的是单个标签,而不是一组标签
* 父标签 parent 和 parents

**获取属性**
myTag.attrs 返回的是一个 Python 字典对象,可以获取和操作这些属性。
如要获取图片的资源位置 src ，myImgTag.attrs["src"]


***
### 异常处理
**两种异常**
1. 网页在服务器上不存在(或者获取页面的时候出现错误)
2. 服务器不存在

**网页不存在**
程序会返回 HTTP 错误，可能是 404 或 500，所有类似情形, urlopen 函数都会抛出“HTTPError”异常。
`
try:
    html = urlopen("http://www.pythonscraping.com/pages/page1.html")
except HTTPError as e:
    print(e)
    # 返回空值,中断程序,或者执行另一个方案
else:
    # 程序继续。注意:如果你已经在上面异常捕捉那一段代码里返回或中断(break),
    # 那么就不需要使用else语句了,这段代码也不会执行
`
**服务器不存在**
urlopen 会返回一个 None 对象，可以增加一个判断语句检测返回的 html 是不是 None
`
if html is None:
    print("URL is not found")
else:
    # 程序继续
`
**标签和子标签不存在**
如果不存在，会发生 AttributeError错误。
如何避免这种的异常，最简单的方式是对两种情形进行检查:
`
try:
    badContent = bsObj.nonExistingTag.anotherTag
except AttributeError as e:
    print("Tag was not found")
else:
    if badContent == None:
        print ("Tag was not found")
    else:
        print(badContent)
`
另一种写法
`
from urllib.request import urlopen
from urllib.error import HTTPError
from bs4 import BeautifulSoup

def getTitle(url):
    try:
        html = urlopen(url)
    except HTTPError as e:
        return None
    try:
        bsObj = BeautifulSoup(html.read())
        title = bsObj.body.h1
    except AttributeError as e:
        return None
    return title

title = getTitle("http://www.pythonscraping.com/pages/page1.html")

if title == None:
    print("Title could not be found")
else:
    print(title)
`

**注意**
在一个异常处理语句中包裹多行语句显然是有危险
* 首先，没办法识别出究竟是哪行代码出现了异常；
* 其次，如果有个页面没有前面的内容，却有后面的其他信息，那么由于前面已经发生异常，后面的信息就不会出现。

***
## 采集

***
## 使用 API

***
## 存储数据

***
## 读取文档

***
## 采集数据清洗

***
## 自然语言处理

***
## 穿越网页表单与登录窗口进行采集

***
## 采集 JavaScript

***
## 图像识别与文字处理

***
## 避开采集陷阱

***
## 用爬虫测试网站

***
## 远程采集
